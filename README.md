# Best NLP Tutorials I ever read

---

This post contains everything I found excellent and interesting about NLP research. Some are tutorials including slides, videos, and blogs, others are some inspiring papers.

P.S. This is not an exhausting list about NLP resources like [awesome-nlp](https://github.com/keon/awesome-nlp), so I try not to put anything I haven't read or articles with depulicated content here. Hope we can all survive from the information flood :)



## Useful Links

- [NLP-progress](https://nlpprogress.com/): Repository to track the progress in NLP, including the datasets and the current state-of-the-art for the most common NLP tasks.




## Introduction
- [Natural Language Processing: State of The Art, Current Trends and Challenges](https://arxiv.org/abs/1708.05148) (Out of Date)
- [Chinese Information Processing Report (2016)](http://cips-upload.bj.bcebos.com/cips2016.pdf): a report discussing both Chinese and English language processing
- [Deep Learning, NLP, and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) by Colah: “The **representation perspective** of deep learning is a powerful view that seems to answer why deep neural networks are so effective”



## Vector Semantics

- A [review](https://zhuanlan.zhihu.com/p/50443871) of vector semantics covering from word2vec to BERT (in Chinese)
- [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) by NSS
- Two awesome tutorials about word2vec by Chris McCormick: [Part 1 is about Skip-gram](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) and [Part 2 is about negative sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)

### Transformer

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) and [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/): breaks the models apart and explains their details
- Why transformer is better than CNN/RNN as feature extraction ([in Chinese](https://zhuanlan.zhihu.com/p/54743941))
- Another [post](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3) by Calvo explains the transformer architecture
- Code example - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) by Harvard NLP group



## Text Mining
- An excellent [tutorial](http://videolectures.net/mlss09uk_blei_tm/) given by David Blei about LDA





## Summarization

- [How to Run Text Summarization with TensorFlow](http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/) by Pavel Surmenok
- [文本自动摘要技术 by 万小军](https://github.com/IsakZhang/NLP-notes/blob/master/Data/%E4%B8%87%E5%B0%8F%E5%86%9B-%E6%96%87%E6%9C%AC%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%E6%8A%80%E6%9C%AF.pdf)





## Question Answering

- [QA in E-commerce Scenarios](./ecommerce-app.md) (also includes search and recommendation in E-com)





## Linguistics

- [Miracles of Human Language: An Introduction to Linguistics](https://www.coursera.org/learn/human-language): a MOOC about linguistics on Coursera
- Book: [Linguistic fundamentals for NLP: 100 essentials from morphology and syntax](https://www.amazon.com/Linguistic-Fundamentals-Natural-Language-Processing/dp/1627050116)


